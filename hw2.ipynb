{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hongdi Li\n",
    "# IST-664"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: import everything need"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.collocations import *\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "import re\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: Read file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "\n",
    "list_text=[]\n",
    "# This list is to help to do with the text part\n",
    "\n",
    "counter=0\n",
    "# a counter\n",
    "with open('16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000002.json', 'r', encoding='utf-8') as file:\n",
    "#open the file\n",
    "\n",
    "\n",
    "        for line in file:\n",
    "        # file is too big to just use load(), to save my laptop, load in by line\n",
    "\n",
    "                data=json.loads(line)\n",
    "\n",
    "                if data[\"text\"] is not None:\n",
    "                   text=data[\"text\"]\n",
    "\n",
    "                   sentences = nltk.sent_tokenize(text)\n",
    "                    #get the sentence from text\n",
    "\n",
    "                   for p in sentences:\n",
    "                        if 'i' in p:\n",
    "                        #only extract those sentences that contain exclamation marks\n",
    "\n",
    "                            text_low=p.lower()\n",
    "                            #lower text\n",
    "\n",
    "                            counter=counter+1\n",
    "                            #number of sentence\n",
    "\n",
    "\n",
    "\n",
    "                            list_text.extend([w for w in word_tokenize(text_low) if re.search('^[a-zA-Z]',w)])\n",
    "                            #save the clean data text in list_text\n",
    "\n",
    "        ## end for loop\n",
    "\n",
    "file.close()\n",
    "## all data loaded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: Get the average length of sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "23.79733743172236"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_text)/counter\n",
    "#to get average length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: Create the tag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "list_pos=[]\n",
    "list_pos =nltk.pos_tag(list_text)\n",
    "# create the tag for each word in list_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: Find adjective phrase by frequency"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "\n",
    "grammar = \"RBADJ: {<RB.*><JJ.*>}\"\n",
    "#define grammar on find adjective phrase\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree=cp.parse(list_pos)\n",
    "\n",
    "###Explain:#################################################################\n",
    "#there only [RB+JJ] or [JJ+NN] to make an adjective phrase\n",
    "#the reason not include [JJ+NN] because it does not give the useful thing\n",
    "\n",
    "#For example, nltk will define [\"white\" as \"JJ\"] and [\"house\" as \"NN\"]\n",
    "#but \"white house\" is actually a word, it also happens on example \"next week\"\n",
    "#will talk more in the report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "rbadj=[]\n",
    "\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() == 'RBADJ':\n",
    "        #to find the find adjective phrase\n",
    "            rbadj.append(subtree)\n",
    "            #save the correct adjective phrase in subtree\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "rb_adj = []\n",
    "rbadj_stop_word=  [\"n't political \",'south lake ','hong kong ','only ibd ','south shore ','threshold type ','so i ']\n",
    "\n",
    "for phrase in rbadj:\n",
    "    i=''\n",
    "    for word, pos in phrase:\n",
    "\n",
    "        i = i+word+' '\n",
    "        #to make tree type to a readable list\n",
    "        #WTS: model 'RB JJ' as output\n",
    "    if i not in rbadj_stop_word:\n",
    "        rb_adj.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "[('as much ', 254),\n ('too early ', 228),\n ('as many ', 210),\n ('more serious ', 188),\n ('late last ', 184),\n ('more severe ', 161),\n ('most important ', 155),\n ('relatively low ', 154),\n ('so many ', 154),\n ('most recent ', 146),\n ('very good ', 145),\n ('extremely well-prepared ', 142),\n ('more likely ', 134),\n ('too much ', 134),\n ('previously unknown ', 132),\n ('so much ', 125),\n ('more related ', 120),\n ('very critical ', 107),\n ('still possible ', 103),\n ('very serious ', 101),\n ('still unknown ', 101),\n ('broadcast rewritten ', 96),\n ('notice commercial ', 96),\n ('very close ', 95),\n ('most popular ', 92),\n ('not clear ', 90),\n ('s first ', 89),\n ('too late ', 87),\n ('very few ', 86),\n ('back negative ', 84),\n ('s largest ', 81),\n ('most likely ', 80),\n ('more important ', 79),\n ('very important ', 77),\n ('as kiwis ', 76),\n ('really good ', 75),\n ('not available ', 75),\n ('very preliminary ', 75),\n ('very strong ', 74),\n ('very low ', 74),\n ('well prepared ', 74),\n ('also involved ', 74),\n ('more dangerous ', 72),\n ('relatively small ', 72),\n ('only able ', 70),\n ('completely new ', 70),\n ('not present ', 70),\n ('far eastern ', 69),\n ('very difficult ', 67),\n ('likely many ', 67)]"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(rb_adj).most_common(50)\n",
    "#frequency 50 top adjective phrase"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: create label to find adjective, nouns, verbs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "adj_tags = {'JJ', 'JJS', 'JJR'}\n",
    "#label of adjective words\n",
    "\n",
    "nn_tags={'NN','NNS','NNP','NNPS'}\n",
    "#label of noun words\n",
    "\n",
    "vb_tags={'VB','VBD','VBG','VBN','VBP','VBZ'}\n",
    "#label of verb words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  TODO: Add the stopwords to clean the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "adj_stop_words=[]\n",
    "nn_stop_words=[]\n",
    "vb_stop_words=[]\n",
    "\n",
    "\n",
    "for w in ['january','u.s.','good','new','more','south','china','more','few','own','chinese','trump','other','white','united','carson','first','last','human','next','wuhan','holiday','nevada','thursday','friday']:\n",
    "    adj_stop_words.append(w)\n",
    "\n",
    "for w in ['quot','state','states','man','today','days','carson','country','countries','quot','day','days','country']:\n",
    "    nn_stop_words.append(w)\n",
    "\n",
    "for w in ['have','are','has','was','said','been','were','had','get','china','don','put','set','keep','got','does','did','make','take','says','told','going','say','come','send','sent','seen','feel']:\n",
    "    vb_stop_words.append(w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: Find adjective, nouns, verbs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "adj_pos=[]\n",
    "nn_pos=[]\n",
    "vb_pos=[]\n",
    "\n",
    "\n",
    "for word,pos in list_pos:\n",
    "    if len(word)>2:\n",
    "        #by looking the data, most len(word)<2 are useless in this case\n",
    "\n",
    "        if (pos in adj_tags):\n",
    "            if word not in adj_stop_words:\n",
    "                adj_pos.append(word)\n",
    "\n",
    "        if (pos in nn_tags):\n",
    "            if word not in nn_stop_words:\n",
    "                nn_pos.append(word)\n",
    "\n",
    "        if (pos in vb_tags):\n",
    "            if word not in vb_stop_words:\n",
    "                vb_pos.append(word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: Get the top 50 adjective, nouns, verbs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "[('global', 9220),\n ('international', 7173),\n ('public', 6664),\n ('medical', 5300),\n ('many', 4115),\n ('novel', 3958),\n ('such', 3914),\n ('due', 3733),\n ('national', 3571),\n ('economic', 3345),\n ('american', 3298),\n ('least', 3214),\n ('confirmed', 2920),\n ('foreign', 2874),\n ('local', 2745),\n ('british', 2616),\n ('chief', 2500),\n ('same', 2473),\n ('severe', 2428),\n ('latest', 2312),\n ('major', 2273),\n ('several', 2273),\n ('central', 2253),\n ('most', 2221),\n ('low', 2170),\n ('potential', 2070),\n ('deadly', 2060),\n ('high', 2039),\n ('recent', 2018),\n ('further', 1984),\n ('possible', 1981),\n ('february', 1974),\n ('close', 1890),\n ('positive', 1886),\n ('outbreak', 1871),\n ('infectious', 1861),\n ('social', 1741),\n ('early', 1739),\n ('financial', 1722),\n ('current', 1688),\n ('second', 1670),\n ('daily', 1643),\n ('green', 1641),\n ('epidemic', 1624),\n ('best', 1616),\n ('likely', 1610),\n ('much', 1572),\n ('past', 1533),\n ('full', 1530),\n ('general', 1524)]"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(adj_pos).most_common(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "[('china', 32712),\n ('coronavirus', 31577),\n ('health', 26746),\n ('virus', 26510),\n ('people', 20915),\n ('cases', 15811),\n ('outbreak', 13582),\n ('world', 10962),\n ('wuhan', 10164),\n ('year', 8323),\n ('emergency', 8092),\n ('flights', 7778),\n ('travel', 7692),\n ('spread', 7608),\n ('government', 7442),\n ('news', 7137),\n ('city', 6898),\n ('disease', 6531),\n ('thursday', 6282),\n ('week', 6029),\n ('time', 5852),\n ('market', 5399),\n ('flight', 5321),\n ('case', 5304),\n ('officials', 5297),\n ('symptoms', 5026),\n ('number', 5007),\n ('president', 4961),\n ('airlines', 4702),\n ('company', 4529),\n ('masks', 4432),\n ('growth', 4286),\n ('organization', 4269),\n ('risk', 4266),\n ('sars', 4255),\n ('home', 4241),\n ('province', 4116),\n ('hospital', 4070),\n ('friday', 4045),\n ('passengers', 3996),\n ('authorities', 3967),\n ('information', 3760),\n ('patients', 3711),\n ('department', 3709),\n ('trade', 3670),\n ('citizens', 3658),\n ('measures', 3651),\n ('control', 3611),\n ('air', 3555),\n ('business', 3546)]"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(nn_pos).most_common(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "[('reported', 6716),\n ('including', 6520),\n ('being', 6162),\n ('confirmed', 5949),\n ('spread', 4476),\n ('according', 4252),\n ('infected', 4229),\n ('declared', 3637),\n ('announced', 3402),\n ('see', 2991),\n ('know', 2902),\n ('added', 2777),\n ('help', 2741),\n ('spreading', 2717),\n ('read', 2625),\n ('taken', 2617),\n ('made', 2559),\n ('working', 2531),\n ('killed', 2484),\n ('continue', 2423),\n ('prevent', 2304),\n ('think', 2270),\n ('expected', 2202),\n ('beijing', 2182),\n ('travel', 2136),\n ('tested', 2103),\n ('taking', 1952),\n ('need', 1884),\n ('came', 1871),\n ('contain', 1865),\n ('wuhan', 1759),\n ('suspended', 1728),\n ('want', 1702),\n ('following', 1695),\n ('asked', 1666),\n ('stop', 1635),\n ('leave', 1626),\n ('remain', 1619),\n ('used', 1613),\n ('fell', 1589),\n ('given', 1573),\n ('issued', 1551),\n ('include', 1541),\n ('continues', 1477),\n ('closed', 1465),\n ('wearing', 1462),\n ('returned', 1461),\n ('work', 1450),\n ('coronavirus', 1435),\n ('called', 1426)]"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(vb_pos).most_common(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
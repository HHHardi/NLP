{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hongdi Li\n",
    "# IST-664"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ***Read me***\n",
    "\n",
    "##  </font>TODO: Import every needed package\n",
    "##  </font>TODO: To make a my stopwords list to help “sort” the data\n",
    "##  </font>TODO: Data Pre-processing\n",
    "##  </font>TODO: Data Analysis\n",
    "###   ---Summary of the data\n",
    "###### ------------  Number of entries\n",
    "###### ------------  Average length of the text\n",
    "###### ------------  Number of countries\n",
    "###### ------------  Number of Author\n",
    "####    ---List the top 50 words by frequency\n",
    "####    ---List the top 50 bigrams by frequencies\n",
    "####    ---List the top 50 bigrams by their Mutual Information scores (using min frequency 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TODO: Import every needed package"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.collocations import *\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: To make a my stopwords list to help \"sort\" the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#This part will explain more in the .doc file\n",
    "\n",
    "stop_set = stopwords.words('english')\n",
    "# https://github.com/igorbrigadir/stopwords/blob/master/en/terrier.txt\n",
    "# The web is to tells which word would be consider as stop word in stopwords.words('english').\n",
    "# For find the frequencies words in next part need use stopwords let us not count some common words like : \"the\", \"and\", ...\n",
    "\n",
    "for w in ['!',',','.',':','?','-s','-ly','</s>','s','\"\"',\"''\",'\"',\"'\",'``','“',')','(','”','’','%','-','[',']','—','}','{',';','\\w','$','\\w+',\"'s\",'’','one','two','said','could','would','us','&','back','also','case','cases','first','last','text','like','get','says','days','week','new','told','may','since','-','--','_','——','–','<','>','=','per','cent','around']:\n",
    "    stop_set.append(w)\n",
    "# However, is not enough to just use the stopwords package. The original .json file is full of dirty information.\n",
    "# such as the data have numbers and single letter, some symbols, and so on.\n",
    "# When I first get top 50 frequencies words there still have many personal think is non-usefull words such as,\"new\",\"would\",\"could\",....\n",
    "# Thus, I put them in my stopwords list as well. I think the homework is asked us to get the useful words instead of some non-useful data.\n",
    "# For this reason, I run the code many time to see what is the really useful data, thus how the stop_set[] come up with.\n",
    "# Also, will write more in the .doc file\n",
    "\n",
    "stop_set_words=[]\n",
    "for w in ['!',',','.',':','?','-s','-ly','</s>','s','\"\"',\"''\",'\"',\"'\",'``','“',')','(','”','’','%','-','[',']','—','}','{',';','\\w','$','\\w+',\"'s\",'’','-','--','_','——','–','<','>','=']:\n",
    "    stop_set_words.append(w)\n",
    "# create a new stopwords set to see how many words in the text, with keep the common words.\n",
    "\n",
    "list_pos=[]\n",
    "tags = set(['JJ'])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: Data Pre-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bengaluru', 'isolation', 'wards', 'hospitals', 'across', 'karnataka', 'helpline', 'take', 'calls', 'coronavirus-related', 'queries', 'ready', 'prevent', 'spread', 'virus', 'india', 'reported', 'kerala', 'yesterday', 'chief', 'secretary', 'state', 'government', 'thursday', 'held', 'meeting', 'additional', 'chief', 'secretary', 'health', 'health', 'commissioner', 'mission', 'director', 'national', 'health', 'mission', 'health', 'department', 'officials', 'reviewed', 'state', 'preparedness', 'tackle', 'coronavirus', 'whenever', 'reported', 'rajiv', 'gandhi', 'institute', 'chest', 'diseases', 'rgicd', 'beds', 'wenlock', 'hospital', 'mangaluru', 'beds', 'selected', 'treatment', 'virus', 'district', 'hospitals', 'five', 'beds', 'isolated', 'patients', 'carrying', 'virus', 'along', 'least', 'ten', 'private', 'hospitals', 'bengaluru', 'setting', 'similar', 'isolation', 'wards', 'national', 'institute', 'virology', 'viral', 'research', 'diagnostic', 'laboratory', 'bangalore', 'medical', 'college', 'research', 'institute', 'roped', 'testing', 'virus', 'next', 'arogya', 'sahayavani', 'helpline', 'run', 'health', 'department', 'take', 'calls', 'related', 'virus', 'udayavani', 'english']\n",
      "['bengaluru', 'isolation', 'wards', 'in', 'hospitals', 'across', 'karnataka', 'and', 'helpline', 'to', 'take', 'calls', 'on', 'coronavirus-related', 'queries', 'are', 'ready', 'to', 'prevent', 'any', 'further', 'spread', 'of', 'the', 'virus', 'after', 'the', 'first', 'case', 'in', 'india', 'was', 'reported', 'from', 'kerala', 'yesterday', 'the', 'chief', 'secretary', 'of', 'the', 'state', 'government', 'on', 'thursday', 'held', 'a', 'meeting', 'with', 'the', 'additional', 'chief', 'secretary', 'health', 'health', 'commissioner', 'mission', 'director', 'of', 'the', 'national', 'health', 'mission', 'and', 'other', 'health', 'department', 'officials', 'and', 'reviewed', 'the', 'state', 'preparedness', 'to', 'tackle', 'any', 'cases', 'of', 'coronavirus', 'whenever', 'reported', 'rajiv', 'gandhi', 'institute', 'of', 'chest', 'diseases', 'rgicd', 'with', '15', 'beds', 'and', 'wenlock', 'hospital', 'at', 'mangaluru', 'with', '10', 'beds', 'have', 'been', 'selected', 'for', 'the', 'treatment', 'of', 'the', 'virus', 'all', 'district', 'hospitals', 'will', 'have', 'five', 'beds', 'isolated', 'for', 'patients', 'carrying', 'the', 'virus', 'along', 'with', 'this', 'at', 'least', 'ten', 'private', 'hospitals', 'in', 'bengaluru', 'will', 'be', 'setting', 'up', 'similar', 'isolation', 'wards', 'the', 'national', 'institute', 'of', 'virology', 'and', 'the', 'viral', 'research', 'and', 'diagnostic', 'laboratory', 'at', 'the', 'bangalore', 'medical', 'college', 'research', 'institute', 'have', 'been', 'roped', 'in', 'for', 'the', 'testing', 'of', 'the', 'virus', 'for', 'the', 'next', 'few', 'days', '104', 'arogya', 'sahayavani', 'helpline', 'run', 'by', 'the', 'health', 'department', 'will', 'take', 'all', 'calls', 'related', 'to', 'the', 'virus', 'udayavani', 'english']\n"
     ]
    }
   ],
   "source": [
    "list_2=[]\n",
    "# To create the empty list to help save the data\n",
    "\n",
    "list_text=[]\n",
    "# This list is to help to do with the text part\n",
    "\n",
    "list_only_text=[]\n",
    "#just with all text\n",
    "\n",
    "country_list=[]\n",
    "#create the list to check if there have the people from same country\n",
    "\n",
    "author_list=[]\n",
    "#create the list to check if there are how many people\n",
    "\n",
    "\n",
    "\n",
    "with open('16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000002.json', 'r', encoding='utf-8') as file:\n",
    "#open the file\n",
    "\n",
    "\n",
    "        for line in file:\n",
    "        # file is too big to just use load(), to save my laptop, load in by line\n",
    "\n",
    "                data=json.loads(line)\n",
    "\n",
    "                list_1=[]\n",
    "                # define and clean the list\n",
    "\n",
    "                ###Explain:\n",
    "                #### Below part is to find the data from the dict in .json file.\n",
    "                #### use if else statement is the first time I saw my .csv file, the format does not right.\n",
    "                #### I thought maybe there have something wrong with the data.\n",
    "                #### Thus, I use the if else to make sure every data I used is from the right dict and will put in the right place.\n",
    "                #### if the data in the dict is empty, I can put the NULL variable in the .csv file.\n",
    "                if data[\"thread\"][\"social\"][\"facebook\"] is not None:\n",
    "                    facebook=data[\"thread\"][\"social\"][\"facebook\"]\n",
    "                    # if the dict is not null, then get the data from the dict\n",
    "                else:\n",
    "                    facebook=''\n",
    "                    # give a Placeholder\n",
    "\n",
    "                if data[\"title\"] is not None:\n",
    "                    title=data[\"title\"]\n",
    "                else:\n",
    "                    title=''\n",
    "\n",
    "                if data[\"published\"] is not None:\n",
    "                    published=data[\"published\"]\n",
    "                else:\n",
    "                    published=''\n",
    "\n",
    "                if data[\"thread\"][\"replies_count\"] is not None:\n",
    "                    replies_count=data[\"thread\"][\"replies_count\"]\n",
    "                else:\n",
    "                    replies_count=''\n",
    "\n",
    "                if data[\"author\"] is not None:\n",
    "                    author=data[\"author\"]\n",
    "\n",
    "                    if author not in author_list:\n",
    "                    # check if have same author\n",
    "                        author_list.append(author)\n",
    "\n",
    "                else:\n",
    "                    author=''\n",
    "\n",
    "                if data[\"url\"] is not None:\n",
    "                    url=data[\"url\"]\n",
    "                else:\n",
    "                    url=''\n",
    "\n",
    "                if data[\"thread\"][\"country\"] is not None:\n",
    "                    country=data[\"thread\"][\"country\"]\n",
    "\n",
    "                    if country not in country_list:\n",
    "                    # check if have the same country\n",
    "                        if country != '':\n",
    "                        # check if have people did not give country\n",
    "                            country_list.append(country)\n",
    "\n",
    "                else:\n",
    "                    country=''\n",
    "\n",
    "                if data[\"text\"] is not None:\n",
    "                   text=data[\"text\"]\n",
    "\n",
    "                   text_low=text.lower()\n",
    "                   # save the copy of text but in all lower case\n",
    "\n",
    "\n",
    "                   \n",
    "                else:\n",
    "                   text=''\n",
    "\n",
    "\n",
    "                list_1.extend((facebook,title,published,replies_count,author,url,country,text))\n",
    "                # put the data in the list_1 like [, , , , , , ,]\n",
    "                # then will clean the list_1 in the begin of the next loop\n",
    "\n",
    "                list_2.append(list_1)\n",
    "                # save the list list_1 in to list_2 to make the structure as\n",
    "                #                      [[, , , , , , ,],\n",
    "                #                       [, , , , , , ,],\n",
    "                #                       [, , , , , , ,],\n",
    "                #                             ...\n",
    "                #                       [, , , , , , ,]]\n",
    "                # This will help me save the data in .csv\n",
    "        \n",
    "                list_only_text.extend([w for w in word_tokenize(text_low) if re.search('^[a-zA-Z0-9]',w) and w not in stop_set_words])\n",
    "                # get all the english text which the total length of the text\n",
    "\n",
    "\n",
    "                list_text.extend([w for w in word_tokenize(text_low) if re.search('^[a-zA-Z]',w) and w not in stop_set])\n",
    "                # to get the text we want from the text_low and save them in list_text\n",
    "                print(list_text)\n",
    "                print(list_only_text)\n",
    "                break\n",
    "                ### Explain:\n",
    "                # Even it makes the time complexity to O(n^2), but I think is ok to do this here.\n",
    "                # the reason is I did put it outside, I save the lower case text data as a .txt file\n",
    "                # but I found is hard for my pc to run it in one time. The .txt data still too big\n",
    "                # I think even is O(n^2), but I do this process by line, is better than put it outside.\n",
    "\n",
    "\n",
    "                #nltk.pos_tag(list_text)\n",
    "\n",
    "                #for word,pos in list_pos:\n",
    "                   # if (pos in tags):\n",
    "                       # new_pos.extend([word,pos])\n",
    "                #print(list_pos)\n",
    "               # i=i+1\n",
    "                #print(i)\n",
    "\n",
    "\n",
    "        ## end for loop\n",
    "\n",
    "file.close()\n",
    "## all data loaded\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "col=[\"facebook\", \"title\", \"published\", \"replies_count\", \"author\", \"url\", \"country\", \"text\"]\n",
    "# create column name\n",
    "\n",
    "with open('Data.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        writer.writerow(col)\n",
    "\n",
    "        writer.writerows(list_2)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO: Data Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary of the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Number of entries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#len(list_2)\n",
    "# also means there 10956 different comments\n",
    "print(list_2)[0:50]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 93,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [93]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#len(list_2)\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# also means there 10956 different comments\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlist_2\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Average length of the text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "len(list_only_text)/len(list_2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "537.5867104782767"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Number of countries and name of these countries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(country_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IN', 'US', 'CA', 'DE', 'GB', 'AU', 'TV', 'PH', 'CN', 'ZA', 'RU', 'SG', 'VE', 'MY', 'NL', 'IE', 'NO', 'FR', 'SB', 'CZ', 'TR', 'PK', 'SO', 'AE', 'KS', 'CO', 'TW', 'LK', 'TH', 'BA', 'NG', 'ES', 'FJ', 'AR', 'BR', 'HK', 'KE', 'UK', 'JP', 'VN', 'ZW', 'NZ', 'IT', 'MO', 'AF', 'MA', 'CH', 'PG', 'MH', 'GR', 'HR', 'KR', 'BG', 'FM', 'MC', 'RO', 'UA', 'HW', 'CL', 'EU', 'SA', 'RW', 'MM', 'MX', 'DK', 'NA', 'LU', 'SE', 'UG', 'AL', 'HU', 'ID', 'MU', 'CW', 'BM', 'AT', 'KH', 'GE', 'JO', 'FI', 'ME', 'BB', 'AM', 'BY', 'EE', 'LV', 'IL', 'BT', 'BE', 'GA', 'PT', 'IR', 'LB', 'VU', 'DM', 'GI', 'TO', 'CR', 'MK', 'CM', 'MT', 'QA', 'PL', 'EC']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "len(country_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "104"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Number of Author"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "3529"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## List the top 50 words by frequency"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "Fre_words=FreqDist(list_text).most_common(50)\n",
    "# use the nltk function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[('china', 45838),\n ('coronavirus', 34402),\n ('virus', 27326),\n ('health', 26792),\n ('people', 21049),\n ('outbreak', 17487),\n ('wuhan', 17479),\n ('chinese', 13860),\n ('spread', 12242),\n ('world', 10996),\n ('thursday', 10269),\n ('travel', 10149),\n ('countries', 9657),\n ('global', 9245),\n ('confirmed', 8907),\n ('u.s.', 8411),\n ('year', 8378),\n ('emergency', 8108),\n ('flights', 7788),\n ('friday', 7731),\n ('public', 7724),\n ('january', 7596),\n ('government', 7454),\n ('news', 7386),\n ('international', 7174),\n ('reported', 6903),\n ('city', 6903),\n ('disease', 6654),\n ('including', 6522),\n ('country', 5877),\n ('time', 5860),\n ('infected', 5669),\n ('market', 5434),\n ('flight', 5322),\n ('symptoms', 5306),\n ('medical', 5301),\n ('officials', 5297),\n ('number', 5032),\n ('united', 5009),\n ('president', 4962),\n ('airlines', 4800),\n ('novel', 4743),\n ('company', 4553),\n ('home', 4551),\n ('masks', 4548),\n ('hospital', 4510),\n ('sars', 4489),\n ('state', 4371),\n ('risk', 4339),\n ('million', 4333)]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fre_words\n",
    "# print the result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## List the top 50 bigrams by frequencies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "Fre_pair = FreqDist(list(nltk.bigrams(list_text))).most_common(50)\n",
    "# by use the nltk function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[(('world', 'health'), 5155),\n (('coronavirus', 'outbreak'), 5027),\n (('public', 'health'), 4445),\n (('novel', 'coronavirus'), 4322),\n (('health', 'organization'), 3824),\n (('health', 'emergency'), 3508),\n (('hong', 'kong'), 3254),\n (('united', 'states'), 3189),\n (('hubei', 'province'), 2683),\n (('travel', 'china'), 2274),\n (('flights', 'china'), 2161),\n (('spread', 'virus'), 2127),\n (('health', 'officials'), 2061),\n (('global', 'health'), 2024),\n (('death', 'toll'), 2020),\n (('global', 'emergency'), 1964),\n (('disease', 'control'), 1885),\n (('south', 'korea'), 1850),\n (('virus', 'spread'), 1771),\n (('mainland', 'china'), 1732),\n (('wuhan', 'china'), 1709),\n (('location', 'location'), 1630),\n (('chinese', 'city'), 1573),\n (('city', 'wuhan'), 1564),\n (('wuhan', 'coronavirus'), 1519),\n (('organization', 'declared'), 1466),\n (('outside', 'china'), 1432),\n (('threshold', 'type'), 1416),\n (('type', 'textpattern'), 1416),\n (('textpattern', 'score'), 1392),\n (('lunar', 'year'), 1371),\n (('rating', 'green'), 1370),\n (('green', 'name'), 1354),\n (('people', 'china'), 1343),\n (('centers', 'disease'), 1342),\n (('control', 'prevention'), 1337),\n (('face', 'masks'), 1326),\n (('killed', 'people'), 1318),\n (('health', 'organisation'), 1247),\n (('respiratory', 'syndrome'), 1197),\n (('american', 'airlines'), 1170),\n (('state', 'department'), 1152),\n (('health', 'authorities'), 1110),\n (('prevent', 'spread'), 1095),\n (('associated', 'press'), 1086),\n (('virus', 'spreading'), 1074),\n (('social', 'media'), 1071),\n (('spread', 'coronavirus'), 1038),\n (('million', 'people'), 1035),\n (('travel', 'trade'), 1028)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fre_pair\n",
    "# print the ans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## List the top 50 bigrams by their Mutual Information scores (using min frequency 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[(('adhanom', 'ghebreyesus'), 11.554707952703428),\n (('tedros', 'adhanom'), 10.913601802882404),\n (('textpattern', 'score'), 10.905413749480928),\n (('rating', 'green'), 10.678697141934741),\n (('pattern', 'count'), 10.361144659503477),\n (('type', 'textpattern'), 10.326422255604463),\n (('count', 'rule'), 10.322658906139612),\n (('threshold', 'type'), 10.266550799627062),\n (('score', 'rating'), 10.233652872505498),\n (('green', 'name'), 10.207690541123405),\n (('severe', 'acute'), 10.062184209204332),\n (('white', 'house'), 10.021595728988082),\n (('death', 'toll'), 9.938292219373452),\n (('respiratory', 'syndrome'), 9.932855511589938),\n (('acute', 'respiratory'), 9.902820520893389),\n (('prime', 'minister'), 9.861419478341737),\n (('hong', 'kong'), 9.853539619484124),\n (('british', 'airways'), 9.830570663545714),\n (('infectious', 'diseases'), 9.819490925334765),\n (('human-to-human', 'transmission'), 9.818676931496054),\n (('fourth', 'quarter'), 9.65561658348257),\n (('associated', 'press'), 9.568706464978877),\n (('donald', 'trump'), 9.510054047379555),\n (('social', 'media'), 9.476191561745914),\n (('tested', 'positive'), 9.430560099003522),\n (('south', 'korea'), 9.246127050695272),\n (('hubei', 'province'), 9.011136891006942),\n (('united', 'states'), 8.926708326081418),\n (('control', 'prevention'), 8.837542894816),\n (('centers', 'disease'), 8.73395321775319),\n (('lunar', 'year'), 8.533176076283496),\n (('location', 'location'), 8.468172520174353),\n (('face', 'masks'), 8.382349138848976),\n (('organization', 'declared'), 8.223464835012706),\n (('close', 'contact'), 7.8973886872833),\n (('american', 'airlines'), 7.879072404236776),\n (('state', 'department'), 7.822675388930939),\n (('disease', 'control'), 7.732449612399002),\n (('international', 'airport'), 7.287491092185427),\n (('international', 'concern'), 7.145548599067876),\n (('chief', 'medical'), 7.0763462018481675),\n (('prevent', 'spread'), 6.911916390856909),\n (('health', 'organisation'), 6.739894758169029),\n (('health', 'organization'), 6.736351382249872),\n (('novel', 'coronavirus'), 6.401727220779655),\n (('global', 'emergency'), 6.386024073213186),\n (('travel', 'trade'), 6.385981501124444),\n (('declared', 'global'), 6.342240871665286),\n (('killed', 'people'), 6.294120172759197),\n (('public', 'health'), 6.099353430716711)]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(list_text)\n",
    "# use the nltk function\n",
    "\n",
    "finder.apply_freq_filter(750)\n",
    "# min is 5, I tired 5-700 and there are many “meaningless” pair, consider the data is super large 750 is good.\n",
    "# I think 750 would be good, such as the top one is Adhanom Ghebreyesus who is working for WHO\n",
    "\n",
    "finder.score_ngrams(bigram_measures.pmi)[:50]\n",
    "# print the ans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'bengaluruisolationwardshospitalsacrosskarnatakahel'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"69.txt\",\"w\",encoding='UTF8') as f:\n",
    "   f.writelines(list_text)\n",
    "f.close()\n",
    "\n",
    "f = open('69.txt',encoding='UTF8')\n",
    "f.read()[0:50]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "list_pos =nltk.pos_tag(list_only_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [94]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word,pos \u001B[38;5;129;01min\u001B[39;00m list_pos:\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (pos \u001B[38;5;129;01min\u001B[39;00m tags):\n\u001B[1;32m----> 4\u001B[0m         new_pos\u001B[38;5;241m.\u001B[39mextend([word,\u001B[43mword\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m])\n",
      "\u001B[1;31mTypeError\u001B[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "new_pos=[]\n",
    "for word,pos in list_pos:\n",
    "    if (pos in tags):\n",
    "        new_pos.extend([word,word+1])\n",
    "\n",
    "                #print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "['coronavirus-related',\n 'ready',\n 'further',\n 'first',\n 'thursday',\n 'additional',\n 'national',\n 'other',\n 'ten',\n 'private',\n 'similar',\n 'national',\n 'viral',\n 'diagnostic',\n 'medical',\n 'next',\n 'few',\n 'arogya',\n 'udayavani',\n 'sure',\n 'new',\n 'chinese',\n 'hubei',\n 'wuhan',\n 'new',\n 'coronavirus',\n 'sick',\n 'bad',\n 'fundamental',\n 'wild',\n 'developed',\n 'dangerous',\n 'stable',\n 'rna',\n 'virulent',\n 'deadly',\n 'asian',\n 'positive',\n 'global',\n 'international',\n 'international',\n 'outbreak',\n 'china',\n 'official',\n 'australian',\n 'positive',\n 'wall',\n 'high',\n 'australian',\n 'major']"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pos[0:50]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}